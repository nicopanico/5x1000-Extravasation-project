# -*- coding: utf-8 -*-
"""
Training script + utility class for the Diagnostic ➜ Extravasation classifier
==========================================================================

Runs end‑to‑end:
1.  Reads the two Excel files generated by our analysis pipeline
2.  Builds the feature matrix (either *primary* or *pruned* feature set)
3.  Repeated Stratified K‑Fold (5×10) cross‑validation on a Logistic
   Regression (Elastic‑Net) – *class_weight='balanced'*
4.  Re‑fit the model on **all** data and finds the optimal operating
   threshold that maximises **F‑β (β = 0.5)** ⇒ favouring **precision**
   (clinically we prefer minimising false‑positive stravasations)
5.  Saves the model (`joblib`) + selected threshold (`.npy`) + PR curve
6.  Provides a lightweight *ExtravasationClassifier* wrapper for runtime
   inference inside `diagnostic_analysis.py`.

Usage from shell (root project dir):
------------------------------------

```bash
python train_extravasation_classifier.py \
    --pos data/risultati_finali.xlsx \
    --neg data/risultati_finali_normali.xlsx \
    --out models/extravasation_logreg.pkl \
    --threshold_out models/extravasation_threshold.npy \
    --pruned   # (optional) use 4‑feature pruned set
```

The script prints repeated‑CV metrics and writes `pr_curve.png` for quick
visual inspection.
"""

import argparse
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (average_precision_score, f1_score,
                             precision_recall_curve, precision_score,
                             recall_score)
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# ---------------------------------------------------------------------------
# Feature sets
# ---------------------------------------------------------------------------
FEATURES_PRIMARY = [
    "ratio_dose",
    "delta_60",
    "delta_120",
    "delta_180",
    "slope_rising",
    "area_after_peak",
]

# Remove highly collinear delta_60/delta_120 keeping delta_180 only
FEATURES_PRUNED = [
    "ratio_dose",
    "delta_180",
    "slope_rising",
    "area_after_peak",
]


# ---------------------------------------------------------------------------
# Data loading helpers
# ---------------------------------------------------------------------------

def _load_feature_df(xls_path: Path) -> pd.DataFrame:
    """Return a DataFrame with all required columns from *Stats* + *Delta*."""
    xls = pd.ExcelFile(xls_path)
    stats = xls.parse("Stats")
    delta = xls.parse("Delta")
    df = stats.merge(
        delta[["base_name", "delta_60", "delta_120", "delta_180"]],
        on="base_name",
        how="inner",
    )
    return df


def build_dataset(pos_path: Path, neg_path: Path, use_pruned: bool = False):
    """Load + concatenate positive/negative cases → (X, y, feature_list)."""
    df_pos = _load_feature_df(pos_path)
    df_pos["label"] = 1
    df_neg = _load_feature_df(neg_path)
    df_neg["label"] = 0
    df = pd.concat([df_pos, df_neg], ignore_index=True)

    features = FEATURES_PRUNED if use_pruned else FEATURES_PRIMARY
    X = df[features].values.astype(float)
    y = df["label"].values.astype(int)
    return X, y, features


# ---------------------------------------------------------------------------
# Training + evaluation
# ---------------------------------------------------------------------------

def train_logistic_regr(X, y, random_state: int = 42):
    """Repeated Stratified K‑Fold CV + final fit on the whole data set."""
    model = Pipeline(
        [
            ("scaler", StandardScaler()),
            (
                "clf",
                LogisticRegression(
                    penalty="elasticnet",
                    solver="saga",
                    l1_ratio=0.5,  # halfway L1/L2 ⇒ automatic feature select
                    class_weight="balanced",
                    max_iter=10000,
                    random_state=random_state,
                ),
            ),
        ]
    )

    rskf = RepeatedStratifiedKFold(
        n_splits=5, n_repeats=10, random_state=random_state
    )

    metrics = {"precision": [], "recall": [], "f1": [], "ap": []}

    for train_idx, test_idx in rskf.split(X, y):
        model.fit(X[train_idx], y[train_idx])
        proba = model.predict_proba(X[test_idx])[:, 1]
        y_pred = (proba >= 0.5).astype(int)
        metrics["precision"].append(precision_score(y[test_idx], y_pred))
        metrics["recall"].append(recall_score(y[test_idx], y_pred))
        metrics["f1"].append(f1_score(y[test_idx], y_pred))
        metrics["ap"].append(average_precision_score(y[test_idx], proba))

    print("\nRepeated Stratified 5×10‑fold CV – mean ± std")
    for k, v in metrics.items():
        v = np.asarray(v)
        print(f"{k:>9}: {v.mean():.3f} ± {v.std():.3f}")

    # Final fit on all data
    model.fit(X, y)
    return model


def choose_threshold(
    model, X, y, beta: float = 0.5, n_points: int = 200, verbose: bool = True
):
    """Pick threshold maximising F‑β (β<1 ⇒ favour precision)."""
    proba = model.predict_proba(X)[:, 1]
    prec, rec, thr = precision_recall_curve(y, proba)
    # The last threshold returned by sklearn is nan -> drop it
    prec, rec, thr = prec[:-1], rec[:-1], thr
    fbeta = (1 + beta ** 2) * prec * rec / (beta ** 2 * prec + rec + 1e-12)
    best_idx = int(np.argmax(fbeta))
    best_thr = thr[best_idx]
    if verbose:
        print(f"\nOptimal threshold for F{beta}: {best_thr:.2f}")
    return best_thr, prec, rec, thr, fbeta


# ---------------------------------------------------------------------------
# Plot helpers
# ---------------------------------------------------------------------------

def save_pr_curve(rec, prec, out_path: Path):
    plt.figure(figsize=(6, 4))
    plt.plot(rec, prec)
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title("Precision‑Recall curve (whole dataset)")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    plt.close()


# ---------------------------------------------------------------------------
# Wrapper class for inference inside DiagnosticAnalysis
# ---------------------------------------------------------------------------
class ExtravasationClassifier:
    """Runtime wrapper: load model + threshold and give binary/probability."""

    def __init__(self, model_path: Path, threshold_path: Path):
        payload = joblib.load(model_path)
        self.model = payload["model"] if isinstance(payload, dict) else payload
        self.features = payload.get("features", FEATURES_PRIMARY)
        self.threshold = float(np.load(threshold_path))

    def predict(self, stats_row: "pd.Series"):
        X = stats_row[self.features].values.reshape(1, -1)
        proba = self.model.predict_proba(X)[0, 1]
        return bool(proba >= self.threshold), proba


# ---------------------------------------------------------------------------
# CLI entry point
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Train Logistic Regression extravasation classifier"
    )
    parser.add_argument("--pos", required=True, help="Excel file – stravaso")
    parser.add_argument(
        "--neg", required=True, help="Excel file – non‑stravaso"
    )
    parser.add_argument(
        "--out", default="extravasation_logreg.pkl", help="Output model path"
    )
    parser.add_argument(
        "--threshold_out",
        default="extravasation_threshold.npy",
        help="Output threshold path",
    )
    parser.add_argument(
        "--pruned",
        action="store_true",
        help="Use pruned 4‑feature set (delta_180 only)",
    )
    args = parser.parse_args()

    X, y, feats = build_dataset(Path(args.pos), Path(args.neg), args.pruned)
    model = train_logistic_regr(X, y)

    thr, prec, rec, thr_raw, fbeta = choose_threshold(model, X, y, beta=0.5)

    # Persist artefacts
    joblib.dump({"model": model, "features": feats}, args.out)
    np.save(args.threshold_out, np.asarray([thr]))
    save_pr_curve(rec, prec, Path("pr_curve.png"))

    print("\nArtifacts saved:")
    print(f"  • model          → {args.out}")
    print(f"  • threshold      → {args.threshold_out} (float32, npy)")
    print(f"  • PR curve plot  → pr_curve.png")


if __name__ == "__main__":
    main()
